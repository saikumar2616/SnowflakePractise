-- ðŸ““ What About Next Time?
    -- What if Agnie wants him to pull in a new log file, every day? That could be a lot of work!
    
    -- How can Kishore automate the movement of the data all the way from the external file through to loading of the enhanced table? Generically, this can be referred to as "production-izing" the data load. 
    -- There are a number of ways to productionize this load process, but we'll start by learning about tasks!

-- ðŸ¥‹ Create a Simple Task
    -- Create a task using the UI in the RAW schema
create task load_logs_enhanced
    warehouse = 'COMPUTE_WH'
    schedule = '5 minute'
    -- <session_parameter> = <value> [ , <session_parameter> = <value> ... ] 
    -- user_task_timeout_ms = <num>
    -- copy grants
    -- comment = '<comment>'
    -- after <string>
    -- when <boolean_expr>
as
    select 'hello';
       
       -- The task you just created doesn't actually do anything, and it's not running anyway (did you see where it says "Suspended"?), but at least you've seen how simple it is to create one, assign a warehouse, and give it a schedule. 


-- ðŸ¥‹ SYSADMIN Privileges for Executing Tasks
use role accountadmin;
--You have to run this grant or you won't be able to test your tasks while in SYSADMIN role
--this is true even if SYSADMIN owns the task!!
grant execute task on account to role SYSADMIN;

use role sysadmin; 

--Now you should be able to run the task, even if your role is set to SYSADMIN
execute task AGS_GAME_AUDIENCE.RAW.LOAD_LOGS_ENHANCED;

--the SHOW command might come in handy to look at the task 
show tasks in account;

--you can also look at any task more in depth using DESCRIBE
describe task AGS_GAME_AUDIENCE.RAW.LOAD_LOGS_ENHANCED;


-- ðŸ““ Running the Task
    -- Once we have a task, we would have to turn it "on" to start the 5 minute clock. We don't want to do that, yet, that's why we executed the task manually. 
    
    -- We can manually run the task any time want, using: 
    -- EXECUTE TASK AGS_GAME_AUDIENCE.RAW.LOAD_LOGS_ENHANCED;


-- ðŸ““ Checking Task History
-- ðŸ¥‹ Execute the Task a Few More Times
--Run the task a few times to see changes in the RUN HISTORY
execute task AGS_GAME_AUDIENCE.RAW.LOAD_LOGS_ENHANCED;

    -- Typos are always common, so check for those, first, of course. In the example below, we have the popular and prevalent "Does not exist" error.  An error like this often means a privileges issue. 


-- ðŸ¥‹ Confirm Any Fixes You Had to Make


-- ðŸ““ Making the Task Better by using Worksheet


-- ðŸŽ¯ Use the CTAS Logic in the Task
    -- We can use the logic from that CTAS statement as the logic for our task.


-- ðŸ““  A Fancy, Important, Serious Task
create or replace task load_logs_enhanced
    warehouse = 'COMPUTE_WH'
    schedule = '5 minute'
    -- <session_parameter> = <value> [ , <session_parameter> = <value> ... ] 
    -- user_task_timeout_ms = <num>
    -- copy grants
    -- comment = '<comment>'
    -- after <string>
    -- when <boolean_expr>
as
    SELECT logs.ip_address
, logs.user_login as GAMER_NAME
, logs.user_event as GAME_EVENT_NAME    
, logs.datetime_iso8601 as GAME_EVENT_UTC
, city
, region
, country
, timezone as GAMER_LTZ_NAME
, convert_timezone('UTC', timezone, logs.datetime_iso8601) as GAME_EVENT_LTZ
, DAYNAME(GAME_EVENT_LTZ) as DOW_NAME
, TOD_NAME
from AGS_GAME_AUDIENCE.RAW.LOGS logs
JOIN IPINFO_GEOLOC.demo.location loc 
ON IPINFO_GEOLOC.public.TO_JOIN_KEY(logs.ip_address) = loc.join_key
AND IPINFO_GEOLOC.public.TO_INT(logs.ip_address) 
BETWEEN start_ip_int AND end_ip_int
JOIN ags_game_audience.raw.time_of_day_lu TOD
on HOUR(GAME_EVENT_LTZ) = TOD.hour;


-- ðŸ¥‹ Executing the Task to TRY to Load More Rows
--make a note of how many rows you have in the table
select count(*)
from AGS_GAME_AUDIENCE.ENHANCED.LOGS_ENHANCED;  --162

--Run the task to load more rows
execute task AGS_GAME_AUDIENCE.RAW.LOAD_LOGS_ENHANCED;

--check to see how many rows were added (if any!)
select count(*)
from AGS_GAME_AUDIENCE.ENHANCED.LOGS_ENHANCED;  --162



-- ðŸŽ¯ Convert Your Task so It Inserts Rows

create or replace task load_logs_enhanced
    warehouse = 'COMPUTE_WH'
    schedule = '5 minute'
    -- <session_parameter> = <value> [ , <session_parameter> = <value> ... ] 
    -- user_task_timeout_ms = <num>
    -- copy grants
    -- comment = '<comment>'
    -- after <string>
    -- when <boolean_expr>
as
    INSERT INTO AGS_GAME_AUDIENCE.ENHANCED.LOGS_ENHANCED  ---newly added insert into table
    SELECT logs.ip_address
, logs.user_login as GAMER_NAME
, logs.user_event as GAME_EVENT_NAME    
, logs.datetime_iso8601 as GAME_EVENT_UTC
, city
, region
, country
, timezone as GAMER_LTZ_NAME
, convert_timezone('UTC', timezone, logs.datetime_iso8601) as GAME_EVENT_LTZ
, DAYNAME(GAME_EVENT_LTZ) as DOW_NAME
, TOD_NAME
from AGS_GAME_AUDIENCE.RAW.LOGS logs
JOIN IPINFO_GEOLOC.demo.location loc 
ON IPINFO_GEOLOC.public.TO_JOIN_KEY(logs.ip_address) = loc.join_key
AND IPINFO_GEOLOC.public.TO_INT(logs.ip_address) 
BETWEEN start_ip_int AND end_ip_int
JOIN ags_game_audience.raw.time_of_day_lu TOD
on HOUR(GAME_EVENT_LTZ) = TOD.hour;

execute task ags_game_audience.raw.load_logs_enhanced;

select * from AGS_GAME_AUDIENCE.ENHANCED.LOGS_ENHANCED ;


-- ðŸ““ Yikes!
    -- The task is working, but the data is piling up and NOT in a good way! 
    -- This is where we learn a new, fancy term: IDEMPOTENCY. 
    -- In short, it means, Kishore can't just write cool stuff that loads data, he has to design a solution that ONLY loads each record one time.
    -- Snowflake has some built in help for IDEMPOTENCY, especially when the file is first picked up from the stage, and we'll talk more about how Snowflake can help with that, but right now we'll focus on making this particular step IDEMPOTENT.  



-- ðŸ““ Dump And Refresh - A Y2K Party!
    --     In the early 2000's, a lot of data engineers would just empty a table and completely reload every single row, totally fresh, every 5 minutes. 
    
    -- Let's empty the table using a truncate, and reload it with the INSERT just like we would have back in the Y2Ks! While you do it this old-school way, feel free to think of this lab work as a sort of Millennial Dance Party. 


-- ðŸ¥‹ Trunc & Reload Like It's Y2K!
--first we dump all the rows out of the table
truncate table ags_game_audience.enhanced.LOGS_ENHANCED;


--then we put them all back in
INSERT INTO ags_game_audience.enhanced.LOGS_ENHANCED (
SELECT logs.ip_address 
, logs.user_login as GAMER_NAME
, logs.user_event as GAME_EVENT_NAME
, logs.datetime_iso8601 as GAME_EVENT_UTC
, city
, region
, country
, timezone as GAMER_LTZ_NAME
, CONVERT_TIMEZONE( 'UTC',timezone,logs.datetime_iso8601) as game_event_ltz
, DAYNAME(game_event_ltz) as DOW_NAME
, TOD_NAME
from ags_game_audience.raw.LOGS logs
JOIN ipinfo_geoloc.demo.location loc 
ON ipinfo_geoloc.public.TO_JOIN_KEY(logs.ip_address) = loc.join_key
AND ipinfo_geoloc.public.TO_INT(logs.ip_address) 
BETWEEN start_ip_int AND end_ip_int
JOIN ags_game_audience.raw.TIME_OF_DAY_LU tod
ON HOUR(game_event_ltz) = tod.hour);

--Hey! We should do this every 5 minutes from now until the next millennium - Y3K!!!
--Alexa, play Yeah by Usher!


-- ðŸ““ Rebuild and Replace
    -- Another method that was used in the early days of data warehousing was a database replace. A whole new warehouse would be built each night and when it was complete, the old one would be given an archival name, and the new one would be given the standard name.
    
    -- Snowflake has the ability to CLONE databases, schemas, tables and more which means this sort of switching out can be done very easily but this isn't really a version of the old-school rebuild and replace. It's pretty significantly different. That's okay, because almost no orgs rebuild their warehouses from scratch each night anymore.
    
    -- On the subject of Snowflake's cloning capabilities, some organizations use cloning to create test and dev copies of entire databases, schemas or just a few of the objects within them.  So, after using modern update methods, you could delete your test and dev instances each night and replace them with fresh clones of your production warehouse. 

    -- Cloning is a very powerful tool, doesn't cost much, and doesn't take long. Cloning is more efficient than copying (you can read more about cloning at docs.snowflake.com).  

    -- We can also use cloning to make back ups of things we feel more comfortable having a safe copy of while we are in heavy development. Let's make a back up copy of our LOGS_ENHANCED table.  We're about to start testing some complex logic and we might want to look back at this table, later. 


-- ðŸ¥‹ Create a Backup Copy of the Table
--clone the table to save this version as a backup
--since it holds the records from the UPDATED FEED file, we'll name it _UF
create table ags_game_audience.enhanced.LOGS_ENHANCED_UF 
clone ags_game_audience.enhanced.LOGS_ENHANCED;



-- ðŸ““ Sophisticated 2010's - The Merge!
    -- After the 2000s, most Data Engineers were moving away from a full truncate and replace. Merge statements started taking center stage. Of course, Merge statements existed before 2010, but it took a minute for them to catch on. 
    
    -- There was a new, more sophisticated way to move! (move...your data...from one place to another)
    
    
    -- A SQL merge lets you compare new records to already loaded records and do different things based on what you learn by doing the comparison. 
    
    -- To define a very simple merge, we'll first figure out:
        -- Where our rows are coming from = our SOURCE. (RAW.LOGS)
        -- Where we want to load our rows to = our TARGET.  (ENHANCED.LOGS_ENHANCED)
        -- We want to add any new rows we find in the source. So we need to figure out which rows are new. 
    
        -- Snowflake Docs gives this sample code for a simple update merge. We can use this as a template to figure out which columns we want to use to match on.
    


-- Here's some merge code we can start with: 
MERGE INTO ENHANCED.LOGS_ENHANCED e
USING RAW.LOGS r
ON r.user_login = e.GAMER_NAME
WHEN MATCHED THEN
UPDATE SET IP_ADDRESS = 'Hey I updated matching rows!';
-- But this code will return an error as "100090 (42P18): Duplicate row detected during DML action" because each gamer_name has more than one row in our table currently (each user has a login and a logout). 

-- What will we need to add to allow our merge to find unique records?



-- ðŸ““ A Working Update Merge
    -- Adding the datetime field was enough to remove the duplicate errors, but we like to be extra safe, so we also added the event name also (e.g. 'login' and 'logoff'). 

MERGE INTO ENHANCED.LOGS_ENHANCED e
USING RAW.LOGS r
ON r.user_login = e.GAMER_NAME
and r.datetime_iso8601 = e.GAME_EVENT_UTC  ---adding this should be enogh to make unique
and r.user_event =e.GAME_EVENT_NAME  -- but added this for safer choice
WHEN MATCHED THEN
UPDATE SET IP_ADDRESS = 'Hey I updated matching rows!';

    -- The number of rows updated will depend on how many times you ran the task and reloaded all the rows. Run a select on your table and see if you really changed all the IP_ADDRESS values!

select * from ENHANCED.LOGS_ENHANCED;
    
    -- If we had not used the CLONE feature to make a copy of the table, we could use TIME TRAVEL to go back to the table right before we wiped out all the IP_ADDRESS values. Check out the documentation for TIME TRAVEL if you are interested in learning more.  The combination of CLONING and TIME TRAVEL can help you fix a lot of otherwise disastrous mistakes!!


-- ðŸ““ Merges Are Powerful
    -- You've just seen a merge example that could be called an UPDATE MERGE. Now we'll write an INSERT MERGE. These aren't really two different things, just two different ways of using a MERGE. 
    
    -- Be aware that you can write very complex merge statements that do lots of things at one time. A single MERGE statement can insert new rows, update changed rows, and delete other rows.
    
    -- For now, we'll continue to keep things simple and separate. Our next merge will simply look for matches, and when it finds that a new row does NOT MATCH anything in our TARGET table, it will add that row to our TARGET table. 



-- ðŸ¥‹ Build Your Insert Merge
    -- Build the new command by cobbling together bits of code from previous statements.

    -- MERGE INTO ENHANCED.LOGS_ENHANCED e
    -- USING () r --we'll put our fancy select here
    -- ON r.user_login = e.GAMER_NAME
    -- and r.datetime_iso8601 = e.game_event_utc
    -- and r.user_event = e.game_event_name
    -- WHEN NOT MATCHED THEN
    -- insert () --list of columns
    -- values () --list of columns (but we can mark as coming from the r select)
    -- ;
    
-- Use the previous MERGE statement and SELECT statement together.  Remember that we use "e" for the "ENHANCED" target table and "r" for the "RAW" source data. 

-- After the select statement, in the JOIN ON and AND lines, update the names of the columns from the "r" data set because you no longer have access to their old names.

-- For the last few statements of INSERT () and VALUES (),  use the menu option to get a list of column names automatically entered into the worksheet for you!

MERGE INTO ENHANCED.LOGS_ENHANCED e
USING (
        SELECT logs.ip_address 
        , logs.user_login as GAMER_NAME
        , logs.user_event as GAME_EVENT_NAME
        , logs.datetime_iso8601 as GAME_EVENT_UTC
        , city
        , region
        , country
        , timezone as GAMER_LTZ_NAME
        , CONVERT_TIMEZONE( 'UTC',timezone,logs.datetime_iso8601) as game_event_ltz
        , DAYNAME(game_event_ltz) as DOW_NAME
        , TOD_NAME
        from ags_game_audience.raw.LOGS logs
        JOIN ipinfo_geoloc.demo.location loc 
        ON ipinfo_geoloc.public.TO_JOIN_KEY(logs.ip_address) = loc.join_key
        AND ipinfo_geoloc.public.TO_INT(logs.ip_address) 
        BETWEEN start_ip_int AND end_ip_int
        JOIN ags_game_audience.raw.TIME_OF_DAY_LU tod
        ON HOUR(game_event_ltz) = tod.hour
    ) r --we'll put our fancy select here
ON r.GAMER_NAME = e.GAMER_NAME
and r.game_event_utc = e.game_event_utc
and r.game_event_name = e.game_event_name
WHEN NOT MATCHED THEN  ---inserts ONLY when do not match
insert (IP_ADDRESS, GAMER_NAME, GAME_EVENT_NAME, GAME_EVENT_UTC, CITY, REGION, COUNTRY, GAMER_LTZ_NAME, GAME_EVENT_LTZ, DOW_NAME, TOD_NAME) --list of columns
values (IP_ADDRESS, GAMER_NAME, GAME_EVENT_NAME, GAME_EVENT_UTC, CITY, REGION, COUNTRY, GAMER_LTZ_NAME, GAME_EVENT_LTZ, DOW_NAME, TOD_NAME) --list of columns (but we can mark as coming from the r select)
;



-- IMPORTANT: Make sure you change the line that says "WHEN MATCHED" to say "WHEN NOT MATCHED".


-- ðŸ““ Testing the Insert Merge
    -- Now that we have an Insert Merge built, we should test it to see if it works. We can start by truncating the table. 
    
    -- The first time we run the merge, it should load EVERY record. The second time we run the merge, it should NOT load ANY records. 


-- ðŸ¥‹ Truncate Again for a Fresh Start
--let's truncate so we can start the load over again
truncate table AGS_GAME_AUDIENCE.ENHANCED.LOGS_ENHANCED;


-- ðŸ¥‹ Check Your Logs Enhanced Table
-- Is it empty? It should be. 
select * from AGS_GAME_AUDIENCE.ENHANCED.LOGS_ENHANCED;   ----0 rows

-- ðŸ¥‹ Run Your Merge Statement which we formed above
    -- Does it load every row?
    -- Because every row is NOT MATCHED, so every row in LOGS should be loaded into LOGS_ENHANCED. 
select * from AGS_GAME_AUDIENCE.ENHANCED.LOGS_ENHANCED;   ----162 rows
    --Run the merge again
select * from AGS_GAME_AUDIENCE.ENHANCED.LOGS_ENHANCED;   ----162 rows ONLY. 0 rows inserted in this MERGE as there are no new records




-- ðŸ““ One Bite at a Time
    -- Did you notice our SQL command is starting to look pretty complex?
    
    -- We built it one piece at a time and we know what each piece does.
    
    -- This is an important lesson for beginning developers. When we encounter complex-looking code, we have to remember that it is nearly always made up of smaller parts that were built and tested one chunk at a time.
    
    -- As Desmond Tutu famously said, The way to eat an elephant is one bite at a time. 


    
-- Room for one more bite? 
    -- Add the lines of code to the top of your MERGE, to make it into a TASK!! Replace your old task with this new version! 

create or replace task load_logs_enhanced
    warehouse = 'COMPUTE_WH'
    schedule = '5 minute'
    -- <session_parameter> = <value> [ , <session_parameter> = <value> ... ] 
    -- user_task_timeout_ms = <num>
    -- copy grants
    -- comment = '<comment>'
    -- after <string>
    -- when <boolean_expr>
as
MERGE INTO ENHANCED.LOGS_ENHANCED e
USING (
        SELECT logs.ip_address 
        , logs.user_login as GAMER_NAME
        , logs.user_event as GAME_EVENT_NAME
        , logs.datetime_iso8601 as GAME_EVENT_UTC
        , city
        , region
        , country
        , timezone as GAMER_LTZ_NAME
        , CONVERT_TIMEZONE( 'UTC',timezone,logs.datetime_iso8601) as game_event_ltz
        , DAYNAME(game_event_ltz) as DOW_NAME
        , TOD_NAME
        from ags_game_audience.raw.LOGS logs
        JOIN ipinfo_geoloc.demo.location loc 
        ON ipinfo_geoloc.public.TO_JOIN_KEY(logs.ip_address) = loc.join_key
        AND ipinfo_geoloc.public.TO_INT(logs.ip_address) 
        BETWEEN start_ip_int AND end_ip_int
        JOIN ags_game_audience.raw.TIME_OF_DAY_LU tod
        ON HOUR(game_event_ltz) = tod.hour
    ) r --we'll put our fancy select here
ON r.GAMER_NAME = e.GAMER_NAME
and r.game_event_utc = e.game_event_utc
and r.game_event_name = e.game_event_name
WHEN NOT MATCHED THEN  ---inserts ONLY when do not match
insert (IP_ADDRESS, GAMER_NAME, GAME_EVENT_NAME, GAME_EVENT_UTC, CITY, REGION, COUNTRY, GAMER_LTZ_NAME, GAME_EVENT_LTZ, DOW_NAME, TOD_NAME) --list of columns
values (IP_ADDRESS, GAMER_NAME, GAME_EVENT_NAME, GAME_EVENT_UTC, CITY, REGION, COUNTRY, GAMER_LTZ_NAME, GAME_EVENT_LTZ, DOW_NAME, TOD_NAME) --list of columns (but we can mark as coming from the r select)
;

-- After creating the task, execute it to check that it succeeds. Like this: 

EXECUTE TASK AGS_GAME_AUDIENCE.RAW.LOAD_LOGS_ENHANCED;

-- What happens if you run it more than once? Does it create multiple copies of each record? Or is the process IDEMPOTENT? 

-- If you'd like to test your MERGE TASK by adding a few fake rows to the source table, you can use the code below. After testing, as long as you keep the user_login field set to something consistent like "fake user", you can easily remove the test rows from your raw data table. 






-- ðŸ¥‹ Testing Cycle (Optional)
--Testing cycle for MERGE. Use these commands to make sure the Merge works as expected

--Write down the number of records in your table 
select * from AGS_GAME_AUDIENCE.ENHANCED.LOGS_ENHANCED; --162 rows

--Run the Merge a few times. No new rows should be added at this time 
EXECUTE TASK AGS_GAME_AUDIENCE.RAW.LOAD_LOGS_ENHANCED; 

--Check to see if your row count changed 
select * from AGS_GAME_AUDIENCE.ENHANCED.LOGS_ENHANCED; --Same 162 rows

--Insert a test record into your Raw Table 
--You can change the user_event field each time to create "new" records 
--editing the ip_address or datetime_iso8601 can complicate things more than they need to 
--editing the user_login will make it harder to remove the fake records after you finish testing 
INSERT INTO ags_game_audience.raw.game_logs 
select PARSE_JSON('{"datetime_iso8601":"2025-01-01 00:00:00.000", "ip_address":"196.197.196.255", "user_event":"fake event", "user_login":"fake user"}');
INSERT INTO ags_game_audience.raw.game_logs 
select PARSE_JSON('{"datetime_iso8601":"2025-01-01 00:00:00.000", "ip_address":"196.197.196.255", "user_event":"fake event", "user_login":"fake user2"}');

--After inserting a new row, run the Merge again 
EXECUTE TASK AGS_GAME_AUDIENCE.RAW.LOAD_LOGS_ENHANCED;

--Check to see if any rows were added 
select * from AGS_GAME_AUDIENCE.ENHANCED.LOGS_ENHANCED;  -- 2new records added--- total 164

--When you are confident your merge is working, you can delete the raw records 
delete from ags_game_audience.raw.game_logs where raw_log like '%fake user%';

--You should also delete the fake rows from the enhanced table
delete from AGS_GAME_AUDIENCE.ENHANCED.LOGS_ENHANCED
where gamer_name = 'fake user';
delete from AGS_GAME_AUDIENCE.ENHANCED.LOGS_ENHANCED
where gamer_name like 'fake user%';

--Row count should be back to what it was in the beginning
select * from AGS_GAME_AUDIENCE.ENHANCED.LOGS_ENHANCED; 