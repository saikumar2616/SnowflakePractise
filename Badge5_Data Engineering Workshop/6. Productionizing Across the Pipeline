-- ðŸ““ Data Engineer Skillset Improvements



-- ðŸ““ Pipeline Improvements
-- His coworkers suggest the following improvements:
    -- 1. He could add some file metadata columns to the load so that he will have a record of what files he loaded and when. 
    -- 2. He could move the logic from the PL_LOGs view into the same SELECT. (fewer pieces to maintain).
    -- 3. If he does change the select logic, he will then need a new target table to accommodate the output of the new select. 
    -- 4. When he has a new select that matches the new target table, he can put it into a new COPY INTO statement. 
    -- 5. After he has a new COPY INTO, he could put it into an Event-Driven Pipeline (instead of a task-based Time-Driven Pipeline)
   
   -- TIP: Don't just run the select listed below. Take some time to look at it. If you have completed Badge 4: Data Lake Workshop, you will recognize some familiar tricks. 



   
-- ðŸ¥‹ A New Select with Metadata and Pre-Load JSON Parsing 
SELECT 
    METADATA$FILENAME as log_file_name --new metadata column
  , METADATA$FILE_ROW_NUMBER as log_file_row_id --new metadata column
  , current_timestamp(0) as load_ltz --new local time of load
  , get($1,'datetime_iso8601')::timestamp_ntz as DATETIME_ISO8601
  , get($1,'user_event')::text as USER_EVENT
  , get($1,'user_login')::text as USER_LOGIN
  , get($1,'ip_address')::text as IP_ADDRESS    
  FROM @AGS_GAME_AUDIENCE.RAW.UNI_KISHORE_PIPELINE
  (file_format => 'ff_json_logs');


-- ðŸŽ¯ Create a New Target Table to Match the Select  (Using CTAS, if you want to)
-- Using CTAS creates the table and loads it in one step, which is cool! But it does something weird. It makes every VARCHAR field VERY big. So if you use the CTAS as a shortcut to define your table, you'll want to tweak the table definition after the initial creation.

Create table  AGS_GAME_AUDIENCE.RAW.ED_PIPELINE_LOGS  as 
SELECT 
    METADATA$FILENAME as log_file_name --new metadata column
  , METADATA$FILE_ROW_NUMBER as log_file_row_id --new metadata column
  , current_timestamp(0) as load_ltz --new local time of load
  , get($1,'datetime_iso8601')::timestamp_ntz as DATETIME_ISO8601
  , get($1,'user_event')::text as USER_EVENT
  , get($1,'user_login')::text as USER_LOGIN
  , get($1,'ip_address')::text as IP_ADDRESS    
  FROM @AGS_GAME_AUDIENCE.RAW.UNI_KISHORE_PIPELINE
  (file_format => 'ff_json_logs');

  -- If you copy the definition and paste it into a worksheet to edit it, you will wipe out all the rows. That's fine. We're going to write a COPY INTO in the next step anyway. 

create or replace TABLE AGS_GAME_AUDIENCE.RAW.ED_PIPELINE_LOGS (
	LOG_FILE_NAME VARCHAR(100),
	LOG_FILE_ROW_ID NUMBER(18,0),
	LOAD_LTZ TIMESTAMP_LTZ(0),
	DATETIME_ISO8601 TIMESTAMP_NTZ(9),
	USER_EVENT VARCHAR(25),
	USER_LOGIN VARCHAR(100),
	IP_ADDRESS VARCHAR(100)
);


-- ðŸ¥‹ Create the New COPY INTO 
--truncate the table rows that were input during the CTAS, if that's what you did
truncate table ED_PIPELINE_LOGS;

--reload the table using your COPY INTO
COPY INTO ED_PIPELINE_LOGS
FROM (
    SELECT 
    METADATA$FILENAME as log_file_name 
  , METADATA$FILE_ROW_NUMBER as log_file_row_id 
  , current_timestamp(0) as load_ltz 
  , get($1,'datetime_iso8601')::timestamp_ntz as DATETIME_ISO8601
  , get($1,'user_event')::text as USER_EVENT
  , get($1,'user_login')::text as USER_LOGIN
  , get($1,'ip_address')::text as IP_ADDRESS    
  FROM @AGS_GAME_AUDIENCE.RAW.UNI_KISHORE_PIPELINE
)
file_format = (format_name = ff_json_logs);



-- ðŸ““ Developing Confidence as a Data Engineer
    -- Does this mean he doesn't have what it takes to be a Data Engineer? Should he have had this syntax memorized already? 
    -- Of course not! Data Engineers use documentation. They use Community forums! They use Stack Overflow! And they do it daily! (Hourly?!?)
    -- Another reason he'll make a great Data Engineer is that he asked his more experienced Data Engineers for input. 
    -- In some organizations fast running pipelines might be the most important thing, while in others, cost might be more important. In yet another, it might always be the goal to have things that are a good compromise between the two. 
    -- Kishore's coworkers explain that there are often no strictly right or wrong design decisions. His role as a Junior Data Engineer will be to know what Snowflake is capable of even if it takes trial and error to implement those capabilities. Over time he'll be expected to combine his knowledge and skills toward accomplishing the goal in a way that meets the organizations priorities.  


-- ðŸ““ Last To-Do on the Improvement List


-- ðŸ““ Event-Driven Pipelines
    -- The pipeline Kishore (and you!) created before is a Time-Driven Pipeline. 
    -- A Time-Driven Pipeline is not always the best solution because it can waste money looking for data that isn't there, or allow a backlog to build up if it hasn't been scheduled correctly to meet demand. 
    -- The major alternative to Time-Driven Pipelines are Event-Driven Pipelines and they are made possible by a Snowflake object called a Snowpipe. 
    -- Our Event-Driven Pipeline will "sleep" until a certain event takes place, then it will wake up and respond to the event. In our case, the "event" we care about is a new file being written to our bucket. When our pipe "hears" that a file has arrived, it will grab the file and move it into Snowflake. 



-- ðŸ““  Review, Progress, and Next Steps
    -- Let's review the components of our old, Task-Driven pipeline, our recent changes, and our end goal.
    -- Remember that we successfully created a Task-Driven or Time-Driven Pipeline that contained 4 steps. 
    -- Then, with the help of the other DEs, Kishore created a COPY INTO that removes the need for Step 3 View logic.
    -- What does the new COPY INTO do to the Step 2 Task? Is that still needed? Or not?
    -- And what about Step 4? Is that still needed? Does it need to be edited? 
    -- With regard to the old Step 2 Task, it will not be needed. It will be replaced with a Snowpipe.  
    -- That COPY INTO Kishore just created will serve as the portion of a Snowpipe we will soon create.
    -- Step 4, the LOAD_LOGS_ENHANCED task, will be edited to point at a different source table and it will be part of our new Event-Driven Pipeline. But that will happen in a later lab. 
    -- For now, with those changes in mind, the image below is a preview of what our new Event-Driven Pipeline will look like when it is complete.  
    -- Now, you may be thinking, what the heck is that HUB thing? What the heck are those flags? And that conveyor belt? What is that?  
    -- Those are cloud engineering infrastructure objects that are leveraged by Snowflake to make continuous loading possible. So before we create the Snowpipe,it will benefit Kishore (and you) to understand how they work. 


-- ðŸ““ Cloud-Based Services for Modern Data Pipelines
    -- Modern data pipelines depend on cloud-based services offered by major cloud providers like Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP). 
    -- Creating an Event-Driven Pipeline in Snowflake depends on 3 types of services created and managed by the cloud providers.  
    
    -- STORAGE
        -- AWS - S3 Buckets
        -- Azure - Blob Storage
        -- GCP - GCS Buckets
        
    -- PUBLISH & SUBSCRIBE NOTIFICATION SERVICES (Hub & Spoke)
        -- AWS - Simple Notification Services (SNS)
        -- Azure - Azure Web PubSub and Azure Event Hub
        -- GCP - Cloud Pub/Sub
        
    -- MESSAGE QUEUING  (Linear Messaging)
        -- AWS - Simple Queue Services (SQS)
        -- Azure - Azure Storage Queues and Azure Service Bus Queues
        -- GCP - Cloud Tasks


-- ðŸ““ A Closer Look at Pub/Sub Services
    -- Publish and Subscribe services are based on a Hub and Spoke pattern. The HUB is a central controller that manages the receiving and sending of messages. The SPOKES are the solutions and services that either send or receive notifications from the HUB. 
    -- If a SPOKE is a PUBLISHER, that means they send messages to the HUB. If a SPOKE is a SUBSCRIBER, that means they receive messages from the HUB. A SPOKE can be both a publisher and a subscriber. 
    -- With messages flowing into and out of a Pub/Sub service from so many places, it could get confusing, fast. So Pub/Sub services have EVENT NOTIFICATIONS and TOPICS. A topic is a collection of event types. A SPOKE publishes NOTIFICATIONS to a TOPIC and subscribes to a TOPIC, which is a stream of NOTIFICATIONS. 


-- ðŸ”­ << See that Symbol? That's a Telescope.
-- ðŸ”­ Kishore's Snowpipe Set
    -- It looks weird. We think it looks more like a plane that is dumping black stuff out as it flies, but, yours might look better, depending on your browser or operating system. 
    
    -- In any event, we want you to slow down for a moment, read carefully, and absorb something important: 
    
    -- For the next few pages, you will be observing a process that we DO NOT WANT YOU TO FOLLOW ALONG WITH. We just want you to WATCH. Please don't post a bunch of comments saying "How do I set up AWS without a Credit Card?" and "You never told us we'd have to have an AWS console account?"  and "It's telling me that uni-kishore-pipeline already exists!" 
    
    -- We're going to tell you a story (and show you screenshots) about the process Kishore follows to set up a Snowpipe using AWS and Snowflake. Then, we'll ask you to follow a shorter set of steps to see a Snowpipe in action, in your own trial account.
    
    -- Don't try to do every step in the process. Wait until you see a ðŸ¥‹ or ðŸŽ¯ before you try to take action. If you reach the end of this lesson and just really, really want to set up your own SNS topic, you can come back to this page and try to muddle through on your own. You might find this stuff helpful as you embark on your self-assigned task, but we will not be offering support for that task. 



-- ðŸ”­ Kishore Logs into His AWS Console Account
    -- You do not need to log in to an AWS Console. There is no requirement to sign up for one. For this section, just read and follow along! 
    -- He goes to SNS and creates a topic called dngw_topic. 
    -- He then goes to the bucket and sets up an Event Notification. The notification is called "a_new_file_is_here." Any time a PUT is run that puts a new file in the bucket, an event notification will be generated. 
    -- What will happen with that notification after it is generated?   It will be sent to to the SNS Topic named dngw_topic. 



-- ðŸ”­ Kishore Gets an SNS IAM Policy and Adds it to His Topic
    -- JUST OBSERVE THIS STEP, DO NOT RUN THIS IN YOUR SNOWFLAKE TRIAL ACCOUNT FOR THIS WORKSHOP. 
    -- If you run the below step in your account before doing the next lab, you will not be able to get the badge for this workshop.  Wait until we tell you to carry out steps. This section is for observation only. 
    -- Kishore ran the command above and Snowflake sent back a policy for him. The policy looks like gibberish at first, but it's pretty simple.
    -- It just says that a User (a Service Account, actually) is allowed to subscribe to the dngw_topic.
    -- When setting up a Snowpipe in your own account (later) you may want to generate a policy like this that you can then copy into a topic but you should not do this step for this Workshop.  
    -- Kishore goes back in to AWS and copies the policy into the dngw_topic. 
    -- He has a few edits to make since he doesn't want to replace the whole policy, he just wants to add a section to it. 


-- ðŸ”­ Kishore Gets an SNS IAM Policy and Adds it to His Topic
    -- JUST OBSERVE THE STEPS ON THIS PAGE, YOU WILL RUN STEPS LIKE THIS IN YOUR NEXT LAB, BUT RIGHT NOW, JUST READ/LOOK/UNDERSTAND. 
    -- Kishore goes back to his Snowflake account and finally runs the command to create a Snowpipe. The AWS_SNS_TOPIC property is a lynchpin and it works like magic behind the scenes. 
    -- After the PIPE is created, Kishore runs a command to get a look at his PIPE. 
    -- The PIPE returns an interesting Key/Value Pair. The Key is "Notification Channel Name" and for Kishore, the value ends in u8Pg. He looks in his AWS Console and notices that his topic has a new subscription and the endpoint property matches the Notification Channel Name for his Snowpipe. His Snowpipe is complete!! 
    
